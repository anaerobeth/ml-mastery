{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn\n",
    "Notes from Lukas Biewald's [Crowdflower Machine Learning class](https://github.com/lukas/ml-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature-extraction-1.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv('tweets.csv')\n",
    "target = df['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "text = df['tweet_text']\n",
    "count_vect=CountVectorizer()\n",
    "count_vect.fit(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `fit` raises `ValueError: np.nan is an invalid document, expected byte or unicode string` as `text` is an object. One solution is to pass the text as string in iterable form : `text = [str(df['tweet_text'])]`. The accepted solution is to retain only non null values as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature-extraction-2.py\n",
    "fixed_target = target[pd.notnull(text)]\n",
    "fixed_text = text[pd.notnull(text)]\n",
    "count_vect.fit(fixed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your first classifier\n",
    "\n",
    "[Naive Bayes](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/) is a classification technique that assumes independence among predictors. For this text classification problem, we will use Multinomial NB and pass in discrete counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifer.py\n",
    "counts = count_vect.transform(text)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(counts, target)\n",
    "nb.predict(count_vect.transform(['i love my iphone']))\n",
    "#=> ['Positive emotion']\n",
    "nb.predict(count_vect.transform(['android or iphone?']))\n",
    "#=> ['No emotion toward brand or product']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build another classifier\n",
    "[Support Vector Machine](http://scikit-learn.org/stable/modules/svm.html) is an efficient classifier that works well for small, clean datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier-svm.py \n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(counts, target)\n",
    "clf.predict(count_vect.transform(['i do not love my iphone']))\n",
    "#=> ['No emotion toward brand or product']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction seems wrong. Scikit-learn default SVC settings are:\n",
    "```\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
    "```\n",
    "\n",
    "Modifying the classifier as: `clf = SVC(class_weight='balanced')` results in a prediction of `[\"I can't tell\"]` for the example above. This classifier does not perform well on other predictions run on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating classifier performance\n",
    "\n",
    "Perform `fit` on training data (first 6000 tweets) then `predict` on testing data (remaining 3092 tweets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test-algorithm-2.py\n",
    "nb.fit(counts[0:6000], target[0:6000])\n",
    "\n",
    "predictions = nb.predict(counts[6000:9092])\n",
    "correct_predictions = sum(predictions == target[6000:9092])\n",
    "print('Percent correct: ', 100.0 * correct_predictions / 3092)\n",
    "#=> Percent correct:  66.3971539457"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a confusion matrix, ignoring \"I can't tell\" for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test-algorithm-3.py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "label_list = ['Positive emotion', 'No emotion toward brand or product', 'Negative emotion']\n",
    "confusion_matrix(target[6000:9092], predictions, labels=label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model performance using [cross validation](http://scikit-learn.org/stable/modules/cross_validation.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test-algorithm-cross-validation.py\n",
    "from sklearn.model_selection import cross_val_score\n",
    "nb = MultinomialNB()\n",
    "\n",
    "scores = cross_val_score(nb, counts, fixed_target, cv=10)\n",
    "scores.mean()\n",
    "#=> 0.648153102333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with a baseline model with [DummyClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier) using predictions based on the most_frequent class value, the cross_val score of the NB model is only slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test-algorithm-cross-validation-dummy.py\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dc = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "scores = cross_val_score(dc, counts, fixed_target, cv=10)\n",
    "scores.mean()\n",
    "#=> 0.592609330138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating other algorithms and hyperparameters\n",
    "\n",
    "[Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifiers can show improved performance by constraining features used to build trees based on multiple samples training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test-algorithm-cross-validation-rf.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "scores = cross_val_score(clf, counts, fixed_target, cv=10)\n",
    "scores.mean()\n",
    "#=> 0.655965236406"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SGD Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) is an estimator that implements SVM with stochastic gradient descent (SGD) learning such that the gradient of the loss is estimated each sample at a time and the model is updated along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test-algorithm-cross-validation-svm.py\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, counts, fixed_target, cv=10)\n",
    "scores.mean()\n",
    "#=> 0.63802686072065051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines, Grid Search and Custom Features\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator using [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.py\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "p = Pipeline(steps=[('counts', CountVectorizer()),\n",
    "                ('multinomialnb', MultinomialNB())])\n",
    "\n",
    "p.fit(fixed_text, fixed_target)\n",
    "p.predict([\"I love my iphone!\"])\n",
    "#=> array(['Positive emotion'], dtype='<U34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline-bigrams.py\n",
    "p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 2))),\n",
    "                ('multinomialnb', MultinomialNB(alpha=0))])\n",
    "p.fit(fixed_text, fixed_target)\n",
    "\n",
    "p.named_steps['counts'].vocabulary_.get(u'garage sale')\n",
    "#=> 18967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline-bigrams-cross-validate.py\n",
    "scores = cross_val_score(p, fixed_text, fixed_target, cv=10)\n",
    "scores.mean()\n",
    "#=> 0.48482069960876162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) to improve estimator accuracy by removing all but the k highest scoring features (SelectKBest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature-selection.py\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 2))),\n",
    "                ('feature_selection', SelectKBest(chi2, k=10000)),\n",
    "                ('multinomialnb', MultinomialNB())])\n",
    "\n",
    "p.fit(fixed_text, fixed_target)\n",
    "scores = cross_val_score(p, fixed_text, fixed_target, cv=10)\n",
    "scores.mean()\n",
    "#=> 0.659039495078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune model parameters using [Grid Search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid-search.py\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "p = Pipeline(steps=[('counts', CountVectorizer()),\n",
    "                ('multinomialnb', MultinomialNB())])\n",
    "\n",
    "parameters = {\n",
    "    'counts__lowercase' : (True, False),\n",
    "    'counts__ngram_range': ((1,1), (1,2)),\n",
    "    'multinomialnb__alpha': (0.5, 1, 2)\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(p, parameters, n_jobs=1, verbose=2, cv=10)\n",
    "grid_search.fit(fixed_text, fixed_target)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "#=> Best score: 0.679\n",
    "#=> Best parameters set:\n",
    "#=> counts__lowercase: True\n",
    "#=> counts__ngram_range: (1, 2)\n",
    "#=> multinomialnb__alpha: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline can be pickled for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline-save.py\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 2))),\n",
    "                ('multinomialnb', MultinomialNB())])\n",
    "\n",
    "p.fit(fixed_text, fixed_target)\n",
    "\n",
    "joblib.dump(p, 'sentiment-model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a demo on how a saved pipeline can be served using [Flask](http://flask.pocoo.org/), see `pipeline-server.py`:\n",
    "<img src=\"sentiment-predictor.png\" width=400 height=150>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:IntroToTensorFlow]",
   "language": "python",
   "name": "conda-env-IntroToTensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
