{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "Notes from Lukas Biewald's [Crowdflower Machine Learning class](https://github.com/lukas/ml-class)\n",
    "\n",
    "### Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11444224/11490434 [============================>.] - ETA: 0s                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "            .XXXXX..        \n",
      "           .XXXX.XXX        \n",
      "          .XX.    X.        \n",
      "          XX.   .XX.        \n",
      "         .XX.  ..XX         \n",
      "         .X.  ..XX.         \n",
      "         XXX..XXXX.         \n",
      "         .XXXXXXX..         \n",
      "          .X...XX           \n",
      "              XX.           \n",
      "             .XX            \n",
      "             XX.            \n",
      "            .XX.            \n",
      "            XXX             \n",
      "           .XX.             \n",
      "          .XX.              \n",
      "          XX.               \n",
      "         .X.                \n",
      "        .X..                \n",
      "        XX                  \n",
      "                            \n",
      "\n",
      "Label:, y_train[idx]\n"
     ]
    }
   ],
   "source": [
    "#digits.py\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "idx = 153\n",
    "digit = X_train[idx]\n",
    "\n",
    "str= \"\"\n",
    "for i in range(digit.shape[0]):\n",
    "    for j in range(digit.shape[1]):\n",
    "        if digit[i][j] == 0:\n",
    "            str += ' '\n",
    "        elif digit[i][j] < 128:\n",
    "            str += '.'\n",
    "        else:\n",
    "            str += 'X'\n",
    "    str += \"\\n\"\n",
    "\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label:\" , y_train[idx])\n",
    "#=> Label: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a perceptron using the flattened digit arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85830929206963147"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras-scikit-learn.py\n",
    "from keras.utils import np_utils\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train = [x.flatten() for x in X_train]\n",
    "perceptron = Perceptron()\n",
    "scores = cross_val_score(perceptron, X_train, y_train, cv=10)\n",
    "scores.mean()\n",
    "#=> 0.85830929206963147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize digit pixel intensities by dividing by the max value of 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perceptron-checkpoint.py\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "img_width = X_train.shape[1]\n",
    "img_height = X_train.shape[2]\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the outputs using [one-hot encoding](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/). This a pre-processing step needed to feed categorical data to many scikit-learn estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_categorical` converts a class vector of integers to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [Sequential](https://keras.io/getting-started/sequential-model-guide/) model by passing a list of layers and specifying an input shape. `Dense` is a fully-connected layer with the specified number of units. `Compile` configures the learning process and accepts an [optimizer](https://keras.io/optimizers/), a [loss](https://keras.io/losses/) function and a list of metrics. For classification problems, set metrics as ['accuracy']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "58976/60000 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8788Epoch 00000: val_acc improved from -inf to 0.91460, saving model to model\n",
      "60000/60000 [==============================] - 3s - loss: 0.4689 - acc: 0.8790 - val_loss: 0.3069 - val_acc: 0.9146\n",
      "Epoch 2/10\n",
      "59744/60000 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9147Epoch 00001: val_acc improved from 0.91460 to 0.92080, saving model to model\n",
      "60000/60000 [==============================] - 2s - loss: 0.3037 - acc: 0.9147 - val_loss: 0.2829 - val_acc: 0.9208\n",
      "Epoch 3/10\n",
      "59456/60000 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9208Epoch 00002: val_acc improved from 0.92080 to 0.92430, saving model to model\n",
      "60000/60000 [==============================] - 2s - loss: 0.2834 - acc: 0.9210 - val_loss: 0.2727 - val_acc: 0.9243\n",
      "Epoch 4/10\n",
      "59488/60000 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9238Epoch 00003: val_acc did not improve\n",
      "60000/60000 [==============================] - 2s - loss: 0.2727 - acc: 0.9238 - val_loss: 0.2734 - val_acc: 0.9223\n",
      "Epoch 5/10\n",
      "59104/60000 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9268Epoch 00004: val_acc did not improve\n",
      "60000/60000 [==============================] - 2s - loss: 0.2660 - acc: 0.9265 - val_loss: 0.2731 - val_acc: 0.9236\n",
      "Epoch 6/10\n",
      "59264/60000 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9271Epoch 00005: val_acc improved from 0.92430 to 0.92480, saving model to model\n",
      "60000/60000 [==============================] - 2s - loss: 0.2615 - acc: 0.9273 - val_loss: 0.2676 - val_acc: 0.9248\n",
      "Epoch 7/10\n",
      "59840/60000 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.9285Epoch 00006: val_acc improved from 0.92480 to 0.92720, saving model to model\n",
      "60000/60000 [==============================] - 2s - loss: 0.2578 - acc: 0.9285 - val_loss: 0.2666 - val_acc: 0.9272\n",
      "Epoch 8/10\n",
      "59232/60000 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9301Epoch 00007: val_acc did not improve\n",
      "60000/60000 [==============================] - 2s - loss: 0.2549 - acc: 0.9299 - val_loss: 0.2717 - val_acc: 0.9250\n",
      "Epoch 9/10\n",
      "59520/60000 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9295Epoch 00008: val_acc improved from 0.92720 to 0.92830, saving model to model\n",
      "60000/60000 [==============================] - 2s - loss: 0.2527 - acc: 0.9296 - val_loss: 0.2650 - val_acc: 0.9283\n",
      "Epoch 10/10\n",
      "59488/60000 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9305Epoch 00009: val_acc did not improve\n",
      "60000/60000 [==============================] - 2s - loss: 0.2504 - acc: 0.9305 - val_loss: 0.2673 - val_acc: 0.9267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126785978>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "model=Sequential()\n",
    "model.add(Flatten(input_shape=(img_width,img_height)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('model', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the final model and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26729143058955668, 0.92669999999999997]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(scores)\n",
    "model.save(\"sequential.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and display the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.08640513,  0.05191677, -0.0533197 , ...,  0.04133975,\n",
      "        -0.02808482, -0.05308444],\n",
      "       [ 0.020721  , -0.04056662,  0.02190006, ..., -0.07330699,\n",
      "         0.07682715, -0.01448923],\n",
      "       [ 0.08531713, -0.03980877,  0.08151612, ...,  0.01148049,\n",
      "        -0.03928037,  0.01881983],\n",
      "       ..., \n",
      "       [ 0.00479493,  0.07352971, -0.04358268, ..., -0.00106092,\n",
      "         0.0591091 ,  0.03954609],\n",
      "       [-0.00603664,  0.07396612, -0.03519871, ..., -0.07693817,\n",
      "         0.03887091, -0.01640342],\n",
      "       [ 0.085405  , -0.05702055,  0.0396649 , ..., -0.03235378,\n",
      "         0.08464644, -0.04457386]], dtype=float32), array([-0.53456676,  0.65486735,  0.14239714, -0.43005994,  0.14697094,\n",
      "        1.39619493, -0.16160907,  0.73810667, -1.49492216, -0.30620834], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "saved_model = load_model(\"sequential.h5\")\n",
    "print(saved_model.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save history of weights and models using [Wandb](https://github.com/wandb/client) callback and visualize log data using [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2575 - acc: 0.9280 - val_loss: 0.2680 - val_acc: 0.9265\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2531 - acc: 0.9301 - val_loss: 0.2711 - val_acc: 0.9259\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2511 - acc: 0.9307 - val_loss: 0.2737 - val_acc: 0.9250\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2497 - acc: 0.9310 - val_loss: 0.2746 - val_acc: 0.9270\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2478 - acc: 0.9319 - val_loss: 0.2732 - val_acc: 0.9260\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2468 - acc: 0.9315 - val_loss: 0.2803 - val_acc: 0.9238\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.2464 - acc: 0.9321 - val_loss: 0.2812 - val_acc: 0.9260\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2448 - acc: 0.9325 - val_loss: 0.2749 - val_acc: 0.9285\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2433 - acc: 0.9328 - val_loss: 0.2811 - val_acc: 0.9253\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.2428 - acc: 0.9331 - val_loss: 0.2780 - val_acc: 0.9280\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import wandb\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "#from wandb.wandb_keras import WandbKerasCallback\n",
    "\n",
    "#run = wandb.init()\n",
    "#config = run.config\n",
    "#TODO:\n",
    "#1. Use config epochs and batch_size as model parameters\n",
    "#2. Add WandKerasCallback to callbacks after Wandb invite is activated\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "        batch_size=10, validation_data=(X_test, y_test),\n",
    "        callbacks=[tensorboard])\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "with open('metrics.json', 'w') as outfile:\n",
    "    json.dump(scores, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Launch TensorBoard `tensorboard --logdir=/path/to/dir` then open `localhost:6006` to on your browser to view the graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tensorboard-sequential-metrics.png\" width=600 height=300>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:IntroToTensorFlow]",
   "language": "python",
   "name": "conda-env-IntroToTensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
